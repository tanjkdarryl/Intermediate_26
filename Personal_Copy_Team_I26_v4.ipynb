{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanjkdarryl/Intermediate_26/blob/main/Personal_Copy_Team_I26_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kOHuw4dEVvW"
      },
      "source": [
        "**Package Installation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aqYHzE3eI9u4"
      },
      "outputs": [],
      "source": [
        "# Ngrok (Python Flask) Method:\n",
        "# !pip install flask-ngrok\n",
        "# !ngrok authtoken 2yM8OQsUHtfI09y48Lmoki1F9Zc_3m9iXgfbv4n1HdpRtaS6Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tjWbjdKRMsGj"
      },
      "outputs": [],
      "source": [
        "# # Install Streamlit related packages\n",
        "# !pip install streamlit\n",
        "# !pip install streamlit streamlit_option_menu\n",
        "# !pip install -q streamlit\n",
        "\n",
        "# # Install localtunnel for exposing the app\n",
        "# !npm install localtunnel\n",
        "\n",
        "# # Create the 'pages' directory where your multi-page app files will reside\n",
        "# !mkdir -p pages\n",
        "\n",
        "# # For Facial Recognition\n",
        "# !pip install torch torchvision torchaudio\n",
        "# !pip install scikit-learn\n",
        "# !pip install matplotlib\n",
        "# !pip install Pillow\n",
        "\n",
        "# # For Audio Feature\n",
        "# !pip install librosa streamlit pandas scikit-learn\n",
        "# !pip install soundfile\n",
        "\n",
        "# # Install Streamlit related packages\n",
        "# !pip install streamlit\n",
        "# !pip install streamlit streamlit_option_menu\n",
        "# !pip install -q streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP52oN04I_kq"
      },
      "source": [
        "**Home Page**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rWMNWo0KtEK",
        "outputId": "f4373275-e60a-4a2a-e8ba-f619bf2630fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Home.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile Home.py\n",
        "import streamlit as st\n",
        "\n",
        "# Set Streamlit page configuration\n",
        "# page_title sets the browser tab title AND the label for this page in the sidebar\n",
        "st.set_page_config(\n",
        "    page_title=\"Home\", # This will be the label for the Home.py page in the sidebar\n",
        "    layout=\"centered\", # Sets the page layout to centered\n",
        "    initial_sidebar_state=\"expanded\", # Keeps the sidebar expanded by default\n",
        ")\n",
        "\n",
        "# --- Content for the Home Page ---\n",
        "st.title(\"Welcome to the Home Page! üè†\")\n",
        "st.write(\"This is the main entry point of our Streamlit application.\")\n",
        "st.markdown(\"\"\"\n",
        "    Hello there! You've landed on the **Home Page**.\n",
        "    Use the sidebar on the left to navigate to other sections.\n",
        "\"\"\")\n",
        "\n",
        "st.info(\"To see the navigation, click the `>` icon in the top-left corner of the page (if the sidebar is hidden).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**About Page**"
      ],
      "metadata": {
        "id": "sQ21OeZ60yyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/1_About.py\n",
        "# import streamlit as st\n",
        "\n",
        "# Set page config for consistency across pages (optional, but good practice)\n",
        "st.set_page_config(layout=\"centered\")\n",
        "\n",
        "st.title(\"About Tommo: Your Digital Companion üí°\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "    At [Your Hackathon Team Name - e.g., Team A.I. for Good], we envision a future where emotional well-being is accessible and proactively supported for everyone.\n",
        "    Introducing **Tommo**, your innovative digital companion designed to elevate mental health support, especially for the disadvantaged and disabled in our society.\n",
        "\"\"\")\n",
        "\n",
        "st.subheader(\"What is Tommo?\")\n",
        "st.markdown(\"\"\"\n",
        "    Tommo, named after the Japanese word for friend ('Tomodachi'), embodies true companionship.\n",
        "    It's more than just an application; it's a reliable touchpoint that helps individuals document their journey towards better mental well-being.\n",
        "    Our solution acts not just as a bridge between an individual and happiness, but also as a secure space to record feelings and expressions, serving as an impetus to better mental health.\n",
        "\"\"\")\n",
        "\n",
        "st.subheader(\"How Tommo Empowers & Assists\")\n",
        "st.markdown(\"\"\"\n",
        "    Tommo leverages cutting-edge AI, including sophisticated **facial and speech recognition** capabilities, to understand and interact with users empathetically.\n",
        "    At its core, a powerful **LLM (Large Language Model) powered by PyTorch** ensures secure, private, and helpful conversations, fostering a genuine sense of connection.\n",
        "\n",
        "    ### Reducing Therapists' Initial Efforts\n",
        "    A core aspect of Tommo's innovation is its ability to support mental health professionals. By consistently recording an individual's emotional states, expressions, and conversational insights over time, Tommo provides therapists with invaluable, objective data. This rich context is available *before* the first session, allowing therapists to:\n",
        "    * **Understand the user's emotional baseline and fluctuations.**\n",
        "    * **Identify recurring patterns or triggers.**\n",
        "    * **Tailor initial interventions more effectively.**\n",
        "    This significantly reduces the initial investigative effort for therapists, enabling them to focus on targeted, impactful therapeutic work from the outset, leading to more efficient and personalized care.\n",
        "\"\"\")\n",
        "\n",
        "st.subheader(\"Our Vision: Empowering Through AI\")\n",
        "st.markdown(\"\"\"\n",
        "    Tommo directly addresses the guiding question of the Hackathon:\n",
        "    _\"How may we improve accessibility and empower the disadvantaged and disabled in our society?\"_\n",
        "    By providing an accessible, non-judgmental, and technologically advanced platform, Tommo empowers individuals to proactively engage with their emotional well-being, breaking down barriers to mental health support.\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnRBKSB40yaK",
        "outputId": "20342b23-c761-420c-de9b-068ae3b64aad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pages/1_About.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Contact Us Page**"
      ],
      "metadata": {
        "id": "MZR9EJhv06oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/2_Contact.py\n",
        "# import streamlit as st\n",
        "\n",
        "st.set_page_config(layout=\"centered\")\n",
        "\n",
        "st.title(\"Connect with the Tommo Team ü§ù\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "    We are passionate about mental wellness and always open to collaboration, feedback, or just a friendly chat about Tommo and the future of AI in mental health.\n",
        "    Your insights are invaluable as we strive to empower communities through innovation.\n",
        "\"\"\")\n",
        "\n",
        "st.subheader(\"General Inquiries\")\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    Feel free to reach out with any questions about Tommo or our project:\n",
        "    * **Email:** [info@tommo-project.com](mailto:info@tommo-project.com)\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "st.subheader(\"Support & Feedback\")\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    If you have specific feedback or require support regarding Tommo's functionality:\n",
        "    * **Email:** [support@tommo-project.com](mailto:support@tommo-project.com)\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "st.subheader(\"Join Our Community & Follow Our Progress\")\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    Stay updated on Tommo's development and connect with our team:\n",
        "    * **Twitter/X:** [@TommoAI_Official](https://twitter.com/TommoAI_Official) (Follow us for updates!)\n",
        "    * **GitHub:** [Tommo-Project/Tommo-App](https://github.com/Tommo-Project/Tommo-App) (Explore our code!)\n",
        "    * **LinkedIn:** [Tommo AI Solutions](https://linkedin.com/company/tommo-ai-solutions) (Connect with our team members!)\n",
        "    * **Our Project Website:** [www.tommo-project.com](https://www.tommo-project.com) (Coming Soon!)\n",
        "    \"\"\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5Q_fzF4079b",
        "outputId": "fc250954-9705-42d3-9bd4-5a76028e0fca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pages/2_Contact.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chatbot Page**"
      ],
      "metadata": {
        "id": "euP6hDAR1bNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/3_Chatbot.py\n",
        "import streamlit as st\n",
        "\n",
        "st.set_page_config(layout=\"centered\")\n",
        "\n",
        "st.title(\"Tommo Chatbot: Your LLM Companion üí¨\")\n",
        "st.write(\"Engage in secure and helpful conversations powered by our PyTorch-based LLM.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8EJ70j-1cdO",
        "outputId": "52465015-526e-45ba-cbb8-52c78fd0f7bd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pages/3_Chatbot.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading RAVDESS dataset from Kaggle (Training)**"
      ],
      "metadata": {
        "id": "-QNiZB5G8f7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!pip install -q kaggle\n",
        "\n",
        "# List available datasets if unsure\n",
        "!kaggle datasets list -s ravdess\n",
        "\n",
        "# Download the dataset\n",
        "!kaggle datasets download -d uwrfkaggler/ravdess-emotional-speech-audio\n",
        "\n",
        "# Unzip it\n",
        "!unzip -q ravdess-emotional-speech-audio.zip -d ravdess"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "Y6aojAZm8e1I",
        "outputId": "6139b0bf-6077-4638-ca53-8f9956e7612c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-878a24ee-60d5-41a2-8b97-04109c4dc0be\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-878a24ee-60d5-41a2-8b97-04109c4dc0be\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n",
            "ref                                                           title                                                      size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "------------------------------------------------------------  --------------------------------------------------  -----------  --------------------------  -------------  ---------  ---------------  \n",
            "uwrfkaggler/ravdess-emotional-speech-audio                    RAVDESS Emotional speech audio                        450102890  2019-01-19 18:28:31.433000          76859        629  0.875            \n",
            "uwrfkaggler/ravdess-emotional-song-audio                      RAVDESS Emotional song audio                          477641670  2019-01-19 21:01:49.663000           3020         49  0.875            \n",
            "orvile/ravdess-dataset                                        Audio-Visual Database (RAVDESS)                     25615024242  2025-04-19 20:46:40.283000            180         22  1.0              \n",
            "dmitrybabko/speech-emotion-recognition-en                     Speech Emotion Recognition (en)                      1035070855  2021-01-25 12:59:50.120000          17306        136  0.875            \n",
            "kartik2khandelwal/speech-emotion-dataset                      RAVDESS as .csv                                          781110  2021-09-15 13:12:51.777000           1307         21  0.5882353        \n",
            "uldisvalainis/audio-emotions                                  Audio emotions                                       1203490156  2020-06-09 12:56:17.030000           5309         40  0.75             \n",
            "uwrfkaggler/ravdess-facial-landmark-tracking                  RAVDESS Facial Landmark Tracking                      466528735  2019-09-10 18:05:59.853000            938         34  0.85294116       \n",
            "cracc97/features                                              MFCCs for Speech Emotion Recognition                   38593913  2021-02-13 18:05:08.413000           1156         15  0.88235295       \n",
            "adrivg/ravdess-emotional-speech-video                         RAVDESS Emotional speech video                      13348818580  2023-02-05 19:13:09.613000            929          6  0.375            \n",
            "dejolilandry/ravdess                                          Ravdess                                               225037056  2021-12-26 04:25:21.923000            142          2  0.4375           \n",
            "preethikurra/ravdess-tess                                     RAVDESS, TESS                                         449036502  2023-03-23 19:48:49.163000            157          2  0.625            \n",
            "tariqblecher/ravdess-8k                                       ravdess_8k                                             45176388  2022-06-25 08:55:24.637000            275          2  0.5              \n",
            "manikantagade/ravdess                                         ravdess                                                78458029  2024-10-21 10:22:31.070000              3          2  0.1875           \n",
            "mostafaabdlhamed/speech-signal-features                       speech recognition features                           762345876  2023-07-01 03:39:46.730000           1186         12  0.5625           \n",
            "thbdh5765/audio-visual-database-of-emotional-speech-and-song  Audio-Visual Database of Emotional Speech and Song  25054405733  2025-03-01 11:31:38.080000            134          9  0.875            \n",
            "ahmedeabozaid/audio-sentiment-analysis                        Audio_Sentiment_Analysis                              935993107  2023-12-01 15:54:03.707000            235         10  0.875            \n",
            "rikinzala/speech-emotion-processed-dataset                    Speech Emotion Processed Dataset                      762230764  2024-11-03 06:49:22.993000            115          7  1.0              \n",
            "borameister/ravdessfractals                                   RAVDESS-Fractals                                        6906647  2019-09-30 13:50:17.160000             26          1  0.1875           \n",
            "bhuviranga/basic-audio-dataset                                Basic Audio Dataset                                     9206040  2023-08-01 10:35:22.850000             60          3  0.8125           \n",
            "ftaham/multi-source-emotion-dataset                           Merged Emotional Speech (RAVDESS,SAVEE,CREMAD)            45471  2024-11-13 13:48:37.233000             37          3  0.64705884       \n",
            "Dataset URL: https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "ravdess-emotional-speech-audio.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "replace ravdess/Actor_01/03-01-01-01-01-01-01.wav? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace ravdess/Actor_01/03-01-01-01-01-02-01.wav? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "DATA_DIR = \"./ravdess\"\n",
        "file_paths = [os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith(\".wav\")]"
      ],
      "metadata": {
        "id": "QBYFrhrJ8pO1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess Audio Data for Emotion Labels**"
      ],
      "metadata": {
        "id": "KcIkMD0z8vMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define path to the unzipped dataset\n",
        "DATASET_PATH = \"ravdess\"\n",
        "\n",
        "# Define a function to extract emotion from filename\n",
        "def extract_emotion(filename):\n",
        "    # Example: 03-01-05-01-01-01-12.wav ‚Üí emotion label is the 3rd value\n",
        "    emotion_map = {\n",
        "        '01': 'neutral',\n",
        "        '02': 'calm',\n",
        "        '03': 'happy',\n",
        "        '04': 'sad',\n",
        "        '05': 'angry',\n",
        "        '06': 'fearful',\n",
        "        '07': 'disgust',\n",
        "        '08': 'surprised'\n",
        "    }\n",
        "    parts = filename.split(\"-\")\n",
        "    return emotion_map.get(parts[2], \"unknown\")\n",
        "\n",
        "# Extract features from all audio files\n",
        "def extract_features(dataset_path):\n",
        "    features = []\n",
        "    for root, _, files in os.walk(dataset_path):\n",
        "      for file in files:\n",
        "        if file.endswith(\".wav\"):\n",
        "            emotion = extract_emotion(file)\n",
        "            filepath = os.path.join(root, file)\n",
        "            y, sr = librosa.load(filepath, duration=3, offset=0.5)\n",
        "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "            mfcc_scaled = np.mean(mfcc.T, axis=0)\n",
        "            features.append([mfcc_scaled, emotion])\n",
        "    return pd.DataFrame(features, columns=['features', 'label'])\n",
        "\n",
        "# Process data\n",
        "data_df = extract_features(DATASET_PATH)\n",
        "#Check data\n",
        "print(\"Number of samples extracted:\", len(data_df))\n",
        "print(data_df['label'].value_counts())\n",
        "\n",
        "# Encode labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "data_df['encoded_label'] = le.fit_transform(data_df['label'])\n",
        "\n",
        "# Final data\n",
        "X = np.array(data_df['features'].tolist())\n",
        "y = np.array(data_df['encoded_label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1BMqFJD8zXb",
        "outputId": "71a0c426-0519-46c8-d0cf-ef6e5c5f2adc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples extracted: 2880\n",
            "label\n",
            "calm         384\n",
            "fearful      384\n",
            "disgust      384\n",
            "happy        384\n",
            "surprised    384\n",
            "sad          384\n",
            "angry        384\n",
            "neutral      192\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train a PyTorch Classifier for emotion classification**"
      ],
      "metadata": {
        "id": "qmxG1VBP9IVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Convert to tensors\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# Train/test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Define a simple neural network\n",
        "class EmotionClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(40, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, len(le.classes_))  # number of classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "model = EmotionClassifier()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(15):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_batch)\n",
        "        loss = loss_fn(output, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vpsks-dE9LxY",
        "outputId": "54cab2c8-e0ba-4a7b-b9c5-846c5b2e099d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 283.2184\n",
            "Epoch 2, Loss: 156.8207\n",
            "Epoch 3, Loss: 149.5917\n",
            "Epoch 4, Loss: 148.0555\n",
            "Epoch 5, Loss: 144.9501\n",
            "Epoch 6, Loss: 142.2334\n",
            "Epoch 7, Loss: 139.0835\n",
            "Epoch 8, Loss: 137.1775\n",
            "Epoch 9, Loss: 135.8031\n",
            "Epoch 10, Loss: 134.5187\n",
            "Epoch 11, Loss: 132.3935\n",
            "Epoch 12, Loss: 130.5488\n",
            "Epoch 13, Loss: 128.4586\n",
            "Epoch 14, Loss: 125.8715\n",
            "Epoch 15, Loss: 123.6817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SImple UI for users to upload .wav audio files**"
      ],
      "metadata": {
        "id": "qyFzsD2I9P2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load model definition\n",
        "class EmotionClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(40, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 8)  # 8 emotion classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Load trained model\n",
        "model = EmotionClassifier()\n",
        "model.load_state_dict(torch.load('emotion_model.pth', map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "# Label decoder (update to match your model)\n",
        "label_map = ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
        "\n",
        "# UI\n",
        "st.title(\"Speech Emotion Detection for Depression Risk Screening\")\n",
        "st.write(\"Upload a `.wav` file and we'll analyze emotional tone.\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Choose a .wav file\", type=\"wav\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    y, sr = librosa.load(uploaded_file, duration=3, offset=0.5)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "    mfcc_scaled = np.mean(mfcc.T, axis=0)\n",
        "    input_tensor = torch.tensor(mfcc_scaled, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        pred_idx = torch.argmax(output).item()\n",
        "        emotion = label_map[pred_idx]\n",
        "\n",
        "    st.success(f\"Detected Emotion: **{emotion}**\")\n",
        "    if emotion in ['sad', 'fearful', 'angry']:\n",
        "        st.warning(\"‚ö†Ô∏è Possible signs of negative emotional tone. Consider further evaluation.\")\n",
        "    else:\n",
        "        st.info(\"üôÇ No immediate signs of depressive tone.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "jqCjfwQC9Rsa",
        "outputId": "65286b2a-8da8-4a7d-fd98-f97e12b7fb46"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'emotion_model.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-3762644101>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Load trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmotionClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'emotion_model.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'emotion_model.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Facial Recognition**"
      ],
      "metadata": {
        "id": "z5w0aF8H2z2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/4_Facial_Recognition.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import io\n",
        "\n",
        "st.set_page_config(layout=\"centered\")\n",
        "\n",
        "st.title(\"Facial Expression Recognition: Understanding Your Expressions üëÅÔ∏è\")\n",
        "st.write(\"Upload an image or use your camera to see predicted facial expressions.\")\n",
        "\n",
        "# --- 1. Define the Model Architecture ---\n",
        "class OptimizedEmotionCNN(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(OptimizedEmotionCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout1 = nn.Dropout2d(0.2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout2 = nn.Dropout2d(0.25)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.relu3 = nn.ReLU(inplace=True)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout3 = nn.Dropout2d(0.3)\n",
        "\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.fc1 = nn.Linear(128, 64)\n",
        "        self.relu_fc = nn.ReLU(inplace=True)\n",
        "        self.dropout_fc = nn.Dropout(0.4)\n",
        "\n",
        "        self.fc_out = nn.Linear(64, num_classes)\n",
        "        self.depression_fc = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout1(self.pool1(self.relu1(self.bn1(self.conv1(x)))))\n",
        "        x = self.dropout2(self.pool2(self.relu2(self.bn2(self.conv2(x)))))\n",
        "        x = self.dropout3(self.pool3(self.relu3(self.bn3(self.conv3(x)))))\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        features = self.dropout_fc(self.relu_fc(self.fc1(x)))\n",
        "\n",
        "        emotion_output = self.fc_out(features)\n",
        "        depression_output = torch.sigmoid(self.depression_fc(features))\n",
        "        return emotion_output\n",
        "\n",
        "# --- 2. Load the Trained Model ---\n",
        "@st.cache_resource\n",
        "def load_emotion_model():\n",
        "    device = torch.device(\"cpu\")\n",
        "    model_path = '/content/drive/MyDrive/Intermediate_26_Resources/optimized_emotion_model_v5.pth'\n",
        "    class_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
        "\n",
        "    model = OptimizedEmotionCNN(num_classes=len(class_names)).to(device)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        model.eval()\n",
        "        st.success(\"Emotion recognition model loaded successfully!\")\n",
        "        return model, class_names\n",
        "    except FileNotFoundError:\n",
        "        st.error(f\"Error: Model file not found at '{model_path}'.\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading model: {e}\")\n",
        "        return None, None\n",
        "\n",
        "model, class_names = load_emotion_model()\n",
        "\n",
        "# --- 3. Image Preprocessing ---\n",
        "transform_inference = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((48, 48)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# --- 4. Prediction Function ---\n",
        "def predict_emotion(image_pil, model, class_names):\n",
        "    if model is None:\n",
        "        return \"Model not loaded.\", None\n",
        "\n",
        "    image_tensor = transform_inference(image_pil).unsqueeze(0).to(torch.device(\"cpu\"))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_tensor)\n",
        "        probabilities = torch.softmax(outputs, dim=1)[0]\n",
        "        predicted_idx = torch.argmax(probabilities).item()\n",
        "\n",
        "    predicted_label = class_names[predicted_idx]\n",
        "    confidence = probabilities[predicted_idx].item() * 100\n",
        "    return predicted_label, confidence\n",
        "\n",
        "# --- 5. Streamlit UI ---\n",
        "if model:\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"Analyze an Image\")\n",
        "\n",
        "    uploaded_file = st.file_uploader(\"Upload an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "    camera_image = None\n",
        "\n",
        "    if uploaded_file is None:\n",
        "        camera_image = st.camera_input(\"Or take a picture...\")\n",
        "\n",
        "    image_to_process = None\n",
        "\n",
        "    if uploaded_file:\n",
        "        image_to_process = Image.open(uploaded_file).convert(\"RGB\")\n",
        "        st.image(image_to_process, caption=\"Uploaded Image\", use_container_width=True)\n",
        "    elif camera_image:\n",
        "        image_to_process = Image.open(io.BytesIO(camera_image.read())).convert(\"RGB\")\n",
        "        st.image(image_to_process, caption=\"Camera Image\", use_container_width=True)\n",
        "\n",
        "    if image_to_process:\n",
        "        with st.spinner(\"Analyzing facial expression...\"):\n",
        "            emotion, confidence = predict_emotion(image_to_process, model, class_names)\n",
        "            st.markdown(f\"### Predicted Emotion: **{emotion}**\")\n",
        "            st.info(f\"Confidence: {confidence:.2f}%\")\n",
        "    else:\n",
        "        st.info(\"Please upload or capture an image to start.\")"
      ],
      "metadata": {
        "id": "8fSCGkq320FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Depression Test**"
      ],
      "metadata": {
        "id": "NmZIiWpmkuQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pages/5_Depression_Test.py\n",
        "import streamlit as st\n",
        "st.set_page_config(page_title=\"PHQ-9 Depression Test\", layout=\"centered\")\n",
        "st.title(\"üß† PHQ-9 Depression Screening\")\n",
        "st.markdown(\"\"\"\n",
        "This questionnaire is used by healthcare professionals to screen for depression.\n",
        "Over the **last 2 weeks**, how often have you been bothered by the following problems?\n",
        "Please answer honestly for an accurate reflection.\n",
        "\"\"\")\n",
        "\n",
        "# PHQ-9 questions and options\n",
        "questions = [\n",
        "    \"Little interest or pleasure in doing things\",\n",
        "    \"Feeling down, depressed, or hopeless\",\n",
        "    \"Trouble falling or staying asleep, or sleeping too much\",\n",
        "    \"Feeling tired or having little energy\",\n",
        "    \"Poor appetite or overeating\",\n",
        "    \"Feeling bad about yourself ‚Äî or that you are a failure or have let yourself or your family down\",\n",
        "    \"Trouble concentrating on things, such as reading the newspaper or watching television\",\n",
        "    \"Moving or speaking so slowly that other people could have noticed? Or the opposite ‚Äî being so fidgety or restless that you have been moving around a lot more than usual\",\n",
        "    \"Thoughts that you would be better off dead or of hurting yourself in some way\"\n",
        "]\n",
        "\n",
        "options = {\n",
        "    \"Not at all (0)\": 0,\n",
        "    \"Several days (1)\": 1,\n",
        "    \"More than half the days (2)\": 2,\n",
        "    \"Nearly every day (3)\": 3\n",
        "}\n",
        "\n",
        "responses = []\n",
        "score = 0\n",
        "\n",
        "with st.form(\"phq9_form\"):\n",
        "    for i, question in enumerate(questions, 1):\n",
        "        st.markdown(f\"**{i}. {question}**\")\n",
        "        choice = st.radio(\n",
        "            label=\"\",\n",
        "            options=list(options.keys()),\n",
        "            key=f\"q{i}\",\n",
        "            index=0\n",
        "        )\n",
        "        responses.append(options[choice])\n",
        "        st.markdown(\"---\")\n",
        "\n",
        "    submitted = st.form_submit_button(\"Submit\")\n",
        "\n",
        "if submitted:\n",
        "    score = sum(responses)\n",
        "    st.header(f\"üìä Total Score: {score} / 27\")\n",
        "\n",
        "    # Interpretation\n",
        "    if score <= 4:\n",
        "        level = \"Minimal or no depression\"\n",
        "        color = \"üü¢\"\n",
        "    elif score <= 9:\n",
        "        level = \"Mild depression\"\n",
        "        color = \"üü°\"\n",
        "    elif score <= 14:\n",
        "        level = \"Moderate depression\"\n",
        "        color = \"üü†\"\n",
        "    elif score <= 19:\n",
        "        level = \"Moderately severe depression\"\n",
        "        color = \"üü†üî¥\"\n",
        "    else:\n",
        "        level = \"Severe depression\"\n",
        "        color = \"üî¥\"\n",
        "\n",
        "    st.subheader(f\"{color} Result: **{level}**\")\n",
        "\n",
        "    # Special attention for question 9\n",
        "    q9_response = responses[8]\n",
        "    if q9_response > 0:\n",
        "        st.error(\"‚ö†Ô∏è **You indicated having thoughts of self-harm.**\\nPlease consider reaching out to a healthcare professional or a crisis hotline immediately.\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"**This tool is not a diagnosis.** Always consult a qualified mental health provider for proper evaluation and support.\")\n",
        "\n",
        "    if score >= 10:\n",
        "        st.info(\"üí° Based on your score, it may be helpful to speak with a therapist.\")\n",
        "    else:\n",
        "        st.success(\"‚úÖ Your score does not indicate clinical depression at this time.\")"
      ],
      "metadata": {
        "id": "LnQ00Llwkudp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run the Streamlit Application**"
      ],
      "metadata": {
        "id": "5MkeDM5P3kCd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sd5GGoA6Pw4l"
      },
      "outputs": [],
      "source": [
        "# Save model\n",
        "torch.save(model.state_dict(), 'emotion_model.pth')\n",
        "\n",
        "# Run the Streamlit app in the background and expose it via localtunnel\n",
        "# The output will include a URL you can open in your browser\n",
        "!streamlit run Home.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}